1
00:00:00,000 --> 00:00:03,764
[MUSIC]

2
00:00:03,764 --> 00:00:08,384
So, in the previous video, we talked
about this concept of using Markov

3
00:00:08,384 --> 00:00:12,970
processes to generate random text
after we train on some data set.

4
00:00:12,970 --> 00:00:16,910
And so, now we're going to dive into
the implementation of this idea.

5
00:00:16,910 --> 00:00:20,580
So by the end of this video, you'll
be able to describe the class design

6
00:00:20,580 --> 00:00:24,710
of the Java classes that we're going
to be using, both in the video and

7
00:00:24,710 --> 00:00:26,960
in your project,
to create a Markov text generator.

8
00:00:26,960 --> 00:00:31,180
And then you'll also be able to implement
some of the code that's needed for

9
00:00:31,180 --> 00:00:32,590
the project.

10
00:00:32,590 --> 00:00:34,710
So let's think back to what we said.

11
00:00:34,710 --> 00:00:39,050
So at the beginning, we're going to build
a model of the patterns that are implicit

12
00:00:39,050 --> 00:00:43,020
in the data that we want to
emulate by training on that data.

13
00:00:43,020 --> 00:00:46,590
And the model that we're building
will have these estates that

14
00:00:46,590 --> 00:00:50,570
represent the current
situation that we're in.

15
00:00:50,570 --> 00:00:54,520
And then transitions between those
states to say how to go forward and

16
00:00:54,520 --> 00:00:56,410
how to generate more and more text.

17
00:00:56,410 --> 00:01:00,160
And then once we've built our model,
we're ready to go into that second stage

18
00:01:00,160 --> 00:01:02,590
of the process and
actually generate some text.

19
00:01:02,590 --> 00:01:05,140
So let's focus the class design first.

20
00:01:05,140 --> 00:01:10,720
So remember that when we talk about class
design we have the notion of interfaces.

21
00:01:10,720 --> 00:01:15,050
And with the interfaces we can
specify what's the desired behavior of

22
00:01:15,050 --> 00:01:18,770
our classes without going into
the details of the implementation.

23
00:01:18,770 --> 00:01:21,480
So when we have a MarkovTextGenerator

24
00:01:21,480 --> 00:01:25,320
what we want it to be able to do is
train on some input data or string.

25
00:01:25,320 --> 00:01:27,480
And then we also wanted to
be able to generate text,

26
00:01:27,480 --> 00:01:30,130
perhaps a certain amount of text,
and so we want to be able to give it

27
00:01:30,130 --> 00:01:33,540
an integer which says the number
of words we want to generate.

28
00:01:33,540 --> 00:01:36,890
Now just to make our
objects more useable and

29
00:01:36,890 --> 00:01:40,600
more flexible, we're also going to
add the functionality of retraining.

30
00:01:40,600 --> 00:01:43,650
So we want to be able to
train on some data and

31
00:01:43,650 --> 00:01:46,820
then maybe generate some text based
on that model, then throw it away and

32
00:01:46,820 --> 00:01:50,580
then retrain on new data
generate more text etc.

33
00:01:50,580 --> 00:01:52,140
So that’s going to be the interface and

34
00:01:52,140 --> 00:01:55,600
these are the methods that we’re
specifying in this interface.

35
00:01:55,600 --> 00:01:59,250
So, if we want to go ahead and implement
that interface we need to define a class

36
00:01:59,250 --> 00:02:01,770
that’s going to implement
each one of these methods.

37
00:02:01,770 --> 00:02:05,090
And let’s focus on
the training piece first.

38
00:02:05,090 --> 00:02:10,420
So, when we train on data,
what we're training on is an input string.

39
00:02:10,420 --> 00:02:13,130
And we're going to go back to the example
from last time where we have that

40
00:02:13,130 --> 00:02:15,590
Beatles song, Hello, Goodbye.

41
00:02:15,590 --> 00:02:20,036
And so we want to think about our
model capturing the quirks and

42
00:02:20,036 --> 00:02:22,270
patterns in that body of text.

43
00:02:22,270 --> 00:02:26,340
So that when we generate new text,
we're going to emulate those quirks and

44
00:02:26,340 --> 00:02:27,640
patterns as well.

45
00:02:27,640 --> 00:02:32,740
So for each new word, what we need
to do is keep track of the current

46
00:02:32,740 --> 00:02:36,960
word that we're looking at and then
what words might follow after that word.

47
00:02:36,960 --> 00:02:40,950
Because our model is going to predict what
next word to generate based on the current

48
00:02:40,950 --> 00:02:43,640
state which we are going to
think of as the current word.

49
00:02:43,640 --> 00:02:48,647
And so what were looking for are likely
consecutive pairs of words, and so

50
00:02:48,647 --> 00:02:53,574
as we process the input string that
we are looking at that's our training

51
00:02:53,574 --> 00:02:58,257
set what we are trying to track is
which words come after other words so

52
00:02:58,257 --> 00:03:01,110
that we can mimic that later.

53
00:03:01,110 --> 00:03:07,430
So, in order to put that into our class
design, our concrete class that implements

54
00:03:07,430 --> 00:03:11,080
the interface MarkovTextGenerator
is going to have a list.

55
00:03:12,170 --> 00:03:15,050
And we're going to use a generic to say,
well, what kind of lists,

56
00:03:15,050 --> 00:03:17,130
what objects are stored in this list.

57
00:03:17,130 --> 00:03:20,860
Well, each object that stored
in this list is a word node.

58
00:03:20,860 --> 00:03:24,020
It's going to be something
that captures what

59
00:03:24,020 --> 00:03:27,540
the information that's contained
in a word ought to be.

60
00:03:27,540 --> 00:03:32,760
And so when we look at a specific word,
what we care about is

61
00:03:32,760 --> 00:03:36,570
what are the possible next
moves after we see that word.

62
00:03:36,570 --> 00:03:40,380
And so we're going to define another
class called WordNode that keeps track of

63
00:03:40,380 --> 00:03:42,460
the value of the word that we're
thinking about, the current word.

64
00:03:42,460 --> 00:03:45,340
And so that's our private String word.

65
00:03:45,340 --> 00:03:50,410
And then it is going to keep a list of the
possible next words that come after it.

66
00:03:51,450 --> 00:03:54,580
Now from a WordNode we
might want some methods.

67
00:03:54,580 --> 00:03:57,090
We might want to get the value
of the current word.

68
00:03:57,090 --> 00:04:02,120
We might need to add a possible next
word as we're training on the data.

69
00:04:02,120 --> 00:04:03,999
If we come across this word again, and

70
00:04:03,999 --> 00:04:07,184
we wanna add a new word to
the collection of possible next moves.

71
00:04:07,184 --> 00:04:12,392
And then when we actually generate text
afterwards, we're going to need it to get

72
00:04:12,392 --> 00:04:17,390
a random next word based on the current
location we are in our state model.

73
00:04:17,390 --> 00:04:21,780
And we'll come back to how we use
that method in a little bit, but

74
00:04:21,780 --> 00:04:26,230
basically what we're doing when we're
training on data is we're going through

75
00:04:26,230 --> 00:04:29,700
the array or
list of words that we have, and

76
00:04:29,700 --> 00:04:34,010
for each one of those words that
we're looking at in our training set.

77
00:04:34,010 --> 00:04:38,932
We have to decide wether we want to create
a new WordNode object that is going to

78
00:04:38,932 --> 00:04:41,140
basically be a state in our model.

79
00:04:41,140 --> 00:04:43,470
So let's get a bit of
a sense of how we do that.

80
00:04:43,470 --> 00:04:46,040
And so
when we start at the very beginning,

81
00:04:46,040 --> 00:04:49,710
we've never seen the word you before,
because this is the very beginning and so

82
00:04:49,710 --> 00:04:54,710
you have to create a new WordNode
object with the value for the word you.

83
00:04:54,710 --> 00:04:59,010
And then we look at what word comes
immediately after you at this point in

84
00:04:59,010 --> 00:05:01,630
the input data set, and
we say that say comes next.

85
00:05:01,630 --> 00:05:06,490
And so the next words list for this
particular instance of the object, for

86
00:05:06,490 --> 00:05:10,170
this particular object
at least contain say.

87
00:05:10,170 --> 00:05:13,610
It's got to have that as
a possible next word to you.

88
00:05:14,640 --> 00:05:18,160
Now we come to the next word in our
training set and it's a word that we

89
00:05:18,160 --> 00:05:22,110
haven't seen before, so we have to create
a new instance of the word node class.

90
00:05:23,170 --> 00:05:26,340
That instance is going to have
the instance variable word

91
00:05:26,340 --> 00:05:32,020
set to the value say and it's possible
next words is going to contain yes.

92
00:05:32,020 --> 00:05:35,130
And so on and so
forth as we go through this list.

93
00:05:35,130 --> 00:05:38,720
And now the first interesting
thing happens when we get to

94
00:05:38,720 --> 00:05:41,210
the second time that we see the word say.

95
00:05:41,210 --> 00:05:43,910
So in the song it goes,
you say yes, I say no.

96
00:05:43,910 --> 00:05:46,570
And so
the second time we see the word say,

97
00:05:46,570 --> 00:05:51,710
we don't want to create a new object that
has the same value for the word in it.

98
00:05:51,710 --> 00:05:55,000
Because what we're trying to do
with this model is keep track of

99
00:05:55,000 --> 00:05:59,430
the distinct possible words that
we might have in our data set, and

100
00:05:59,430 --> 00:06:02,590
for each one of those,
what are all possible next words.

101
00:06:02,590 --> 00:06:06,910
And so what we'd like to do is update the
object that we already created whose value

102
00:06:06,910 --> 00:06:11,950
for the variable word was say, and we want
to add a new possible next word to it.

103
00:06:11,950 --> 00:06:14,562
Now say is followed by the word no.

104
00:06:14,562 --> 00:06:19,150
All right and so that's what we did
with the object whose value for

105
00:06:19,150 --> 00:06:20,460
the variable word was say.

106
00:06:21,580 --> 00:06:22,200
All right.

107
00:06:22,200 --> 00:06:25,230
So we keep on going and
we keep on going, but

108
00:06:25,230 --> 00:06:29,120
remember that our model actually had
this notion of probabilities, right?

109
00:06:29,120 --> 00:06:31,370
When we were thinking
about the Markov model,

110
00:06:31,370 --> 00:06:36,450
what we had was each word defined as
state and then we had probabilities.

111
00:06:36,450 --> 00:06:40,320
On the transitions for how likely was it

112
00:06:40,320 --> 00:06:44,840
each of the possible next words
was actually going to be given?

113
00:06:44,840 --> 00:06:47,740
So, if we have the word hello
at the end of a sentence, so

114
00:06:47,740 --> 00:06:52,300
hello with a period, half the time, we
a re going to have the word I afterwards,

115
00:06:52,300 --> 00:06:54,900
but a sixth of the time,
we are going to have the word why, or

116
00:06:54,900 --> 00:06:57,750
a third of the time we are going
to have the word you afterwards.

117
00:06:57,750 --> 00:07:02,600
And it's not clear that we've captured
that in our algorithm, so far,

118
00:07:02,600 --> 00:07:07,500
of training through the data and just
adding words to the list of next words.

119
00:07:07,500 --> 00:07:09,800
But let's think about what we've done.

120
00:07:09,800 --> 00:07:10,470
And in particular,

121
00:07:10,470 --> 00:07:15,680
let's see what happens to the object
who's word points to the word hello.

122
00:07:15,680 --> 00:07:18,800
So, the first time we encounter
hello in our song, and

123
00:07:18,800 --> 00:07:21,730
you can go through the lyrics and
see, it's followed by I.

124
00:07:21,730 --> 00:07:28,000
And so the list nextWords gets the value,
or gets I put into it.

125
00:07:28,000 --> 00:07:30,680
And then the next time I comes again.

126
00:07:30,680 --> 00:07:35,400
And notice that we just insert a second
copy of the string I into this list.

127
00:07:35,400 --> 00:07:36,050
Okay?

128
00:07:36,050 --> 00:07:41,130
Next time we insert why, and
next time we insert I again, and

129
00:07:41,130 --> 00:07:44,700
then you, and
then the next time will be a you as well.

130
00:07:44,700 --> 00:07:48,760
And so as we're training on the data set,
what we're doing is we're

131
00:07:48,760 --> 00:07:53,090
concerned about whether the current word
that we're looking at is new or not.

132
00:07:53,090 --> 00:07:55,930
And every time we've got a new word,
then we add a new object.

133
00:07:55,930 --> 00:07:59,550
But then when we look up what the next
word is, we don't care if we've seen it

134
00:07:59,550 --> 00:08:02,725
before, we always just stick
it into the list of nextWords.

135
00:08:03,790 --> 00:08:06,560
And that's gonna be really important,
because then when we go ahead and

136
00:08:06,560 --> 00:08:12,820
generate text, what we're going to do, is
when we generate the next random word for

137
00:08:12,820 --> 00:08:17,440
a particular word, we're just
going to pick randomly, uniformly,

138
00:08:17,440 --> 00:08:22,040
between all of the words
in our next word's list.

139
00:08:22,040 --> 00:08:25,210
And that's gonna do exactly what we
need in terms of the probabilities

140
00:08:25,210 --> 00:08:29,880
because the probability of picking
I from a list of six elements,

141
00:08:29,880 --> 00:08:32,560
three of which are I is
exactly three out of six.

142
00:08:32,560 --> 00:08:37,060
If we pick each one of those elements with
equal probability, and so what we've done

143
00:08:37,060 --> 00:08:41,840
is we've stacked the list of next words so
that the distribution of probabilities for

144
00:08:41,840 --> 00:08:45,150
each distinct word matches
exactly what we need.

145
00:08:45,150 --> 00:08:48,240
And so that simple algorithm
that we talked about where,

146
00:08:48,240 --> 00:08:53,010
when we process each word we just stick
the next word into the list of next words,

147
00:08:53,010 --> 00:08:57,130
not caring if we've seen it before or
not, does exactly the right thing for

148
00:08:57,130 --> 00:09:01,570
generating the next words with
the correct probabilities.

149
00:09:01,570 --> 00:09:06,220
So now let's talk about how we would
Implement this algorithm at a high level,

150
00:09:06,220 --> 00:09:08,830
so we want to think about
how we generate words

151
00:09:08,830 --> 00:09:10,680
using the model that we've already built.

152
00:09:10,680 --> 00:09:14,190
So we have at our disposal
a list of word nodes.

153
00:09:14,190 --> 00:09:18,020
And now what we'd like to do is use that
list of word nodes to generate some text.

154
00:09:18,020 --> 00:09:20,980
And so until we have enough words,
because remember we've got

155
00:09:20,980 --> 00:09:25,570
that integer argument that tells us how
many words we're supposed to produce,

156
00:09:25,570 --> 00:09:30,960
what we're going to do is look for
the current word that we're currently at

157
00:09:30,960 --> 00:09:34,510
through the list of word
notes that we have.

158
00:09:34,510 --> 00:09:39,150
We're going to look for which word
note has that word as its value,

159
00:09:39,150 --> 00:09:44,050
and in that node, we're going to
generate a random number between 0 and

160
00:09:44,050 --> 00:09:47,850
the number of words in
that list of nextWords.

161
00:09:47,850 --> 00:09:52,080
And we can generate random numbers in
Java because we have this great package

162
00:09:52,080 --> 00:09:53,070
called random.

163
00:09:53,070 --> 00:09:53,860
And so

164
00:09:53,860 --> 00:09:58,960
we just pick a random number between 0 and
the size of the list, minus 1.

165
00:09:58,960 --> 00:10:04,180
And just pick the next word whose
index is that number, and so each of

166
00:10:04,180 --> 00:10:08,780
the possible next words in that list will
have equal probability of being picked.

167
00:10:08,780 --> 00:10:10,400
And because we've stacked the list,

168
00:10:10,400 --> 00:10:12,740
then we'll get the right
distribution that we want.

169
00:10:12,740 --> 00:10:14,930
And so
we'll just print the word at that index.

170
00:10:14,930 --> 00:10:18,570
We update what the currentWord is and
we look for the currentWord node, and

171
00:10:18,570 --> 00:10:20,910
we just keep on going and keep on going.

172
00:10:20,910 --> 00:10:24,170
And now you're ready to
begin your project and

173
00:10:24,170 --> 00:10:27,760
implement this high
level algorithm in Java.

174
00:10:27,760 --> 00:10:33,030
So enjoy the class design,
enjoy generating some really cool text.

175
00:10:33,030 --> 00:10:35,730
And play around with what starter
text you want to train on,

176
00:10:35,730 --> 00:10:37,460
what training data sets you'll have.

177
00:10:37,460 --> 00:10:41,700
And you'll notice that this
can be a little bit absorbing.

178
00:10:41,700 --> 00:10:43,390
Hours of fun can be had.